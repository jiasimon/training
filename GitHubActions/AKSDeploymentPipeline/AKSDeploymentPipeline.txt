Azure Kubernetes Service deployment pipeline with GitHub Actions
https://learn.microsoft.com/en-us/training/modules/aks-deployment-pipeline-github-actions/

• Describe a continuous integration and continuous deployment process that uses GitHub Actions.
• Create a deployment pipeline by using GitHub Actions and Azure.
• Deploy a cloud-native application to AKS by using GitHub Actions.


Suppose that you lead the IT development team for Contoso Video, a video production company whose technology stack has migrated to Azure Kubernetes Service (AKS). 


CI/CD pipelines

1. What's the difference between CI and CD?
A: CI is related to integration and testing, and CD is related to packaging and deployment.
CI is related to the integration of a team's code, and CD is related to the packaging and deployment of the code.

2. What's a CI pipeline?
A: A set of processes that runs automatically when a new code change is detected.
The set of processes that are automatically triggered by a code change is called a CI pipeline.


Design the pipeline
Triggers

Step 1: Clone the repo


Step 2: Build the image


Step 3: Push the image to a container registry


Step 4: Deploy the application




Exercise - Set up the project

1. Fork the sample repository to your GitHub account. You don't have write permissions to the original sample repository, so you need to fork the repository to create your own GitHub Actions pipeline.

https://github.com/MicrosoftDocs/mslearn-aks-deployment-pipeline-github-actions

https://github.com/jiasimon/mslearn-aks-deployment-pipeline-github-actions


2. Sign in to Azure Cloud Shell with the Azure subscription you want to deploy resources to. When Cloud Shell opens, make sure Bash is selected as the shell to run.
3. Clone 
git clone https://github.com/jiasimon/mslearn-aks-deployment-pipeline-github-actions

4. cd mslearn-aks-deployment-pipeline-github-actions

5. Run the following command to execute the init.sh file located in the root of the project:
bash init.sh

6. When the script finishes running, it outputs a list of variables. Copy and store the variable values to use in future exercises.
  • Resource Group Name
  • ACR Name
  • ACR Login Username
  • ACR Password
  • AKS DNS Zone Name


Run the following Azure CLI command to check whether the resource group shown in the script output is listed.
az group list -o table

Run the following command to check whether the Container Registry instance shown in the script output is listed.
az acr list -o table



Container images
Note: The docker build command doesn't work within the Cloud Shell environment, because the use of Docker inside a running container is disallowed. If you want to test the docker build . command, clone the repository locally and run the command by using your own Docker installation.

Image tags
Tags are an important aspect of working with container images. Tags can tell you the type of image and differentiate between multiple images that have the same name

1. What's a Docker image?
A: A standalone, lightweight package that includes all you need to build and run an application in a container.


2. How can a CI/CD pipeline use image tags?
A: To create and deploy different versions of application images that have the same name.




Exercise - Build the application image

Create the GitHub Actions workflow
• Select the Actions tab.
• Select the link to set up a workflow yourself.
At this point, the pipeline is just a blank file in the .github/workflows directory in your repository. GitHub provides the prebuilt components that you need to build most of the pipelines. To get started, copy and paste the following code into the Edit new file pane:
# This is a basic workflow to help you get started with Actions

name: CI

# Controls when the action will run. Triggers the workflow on push or pull request
# events but only for the main branch
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      # Runs a single command using the runners shell
      - name: Run a one-line script
        run: echo Hello, world!

      # Runs a set of commands using the runners shell
      - name: Run a multi-line script
        run: |
          echo Add other actions to build,
          echo test, and deploy your project.


Above the Edit new file pane, rename the file from main.yml to build-staging.yml.

Change the name key from CI to Build and push the latest build to staging.
# This is a basic workflow to help you get started with Actions

name: Build and push the latest build to staging


Modify the 'on' trigger
The basic workflow template comes with two triggers:
  • Any push to the main branch.
  • Any pull request on the main branch.


You don't need the pipeline to run on a pull request, so modify it to keep only the push trigger by changing the triggers in the on key. Remove the second trigger and leave only the push tags.

name: Build and push the latest build to staging

on:
  push:
    branches: [ main ]



Next, start working on the job steps. In this process, you implement both the build tasks and the deploy tasks in your pipeline design diagram.
  1. Under jobs, rename the build key to build_push_image.
  2. You want this workflow to run on Ubuntu 20.04, so change the runs-on key from ubuntu-latest to ubuntu-20.04.
  3. Delete the last two commands in the steps key, which are just examples for the template.
Your file, without the comments, should look like this example:
name: Build and push the latest build to staging

on:
  push:
    branches: [ main ]

jobs:
  build_push_image:
    runs-on: ubuntu-20.04

    steps:
      - uses: actions/checkout@v2



Add Docker steps
1. In the Marketplace tab in the right panel, search for docker login, and select the first result published by docker.
2. Under Installation, select the copy icon to copy the usage YAML.
3. Paste the copied YAML below the actions/checkout@v2 action.
Note: 
Be careful with indentation when you use YAML. The name key should be aligned with the preceding uses key.

4. Add the following values to the registry, username, and password keys:
• registry: ${{ secrets.ACR_NAME }}
• username: ${{ secrets.ACR_LOGIN }}
• password: ${{ secrets.ACR_PASSWORD }}

5. Delete the other keys, because they aren't used in this exercise.
6. In the right panel under Marketplace, search for build and push docker images, and select the first result published by docker.
7. Under Installation, select the copy icon to copy the usage YAML.
8. Paste the copied YAML below the last key from the previously copied docker-login action.
9. Rename the name key from Build and push Docker images to Build and push staging images.
10. Add the following values to the context, push, and tags keys:
  • context: .
  • push: true
  • tags: ${{secrets.ACR_NAME}}/contoso-website:latest
11. Delete the other keys, because they aren't used in this exercise.
12. Add another action called docker/setup-buildx-action between the checkout action and the login action, to set up the build engine for Docker to use. Copy the following snippet and paste it between the checkout and login actions.

- name: Set up Buildx
  uses: docker/setup-buildx-action@v3.0.0


To commit your changes, select the Commit changes button at upper right. On the Commit changes screen, enter a description for the commit, and then select Commit changes.

Set the secrets
To set the secrets, on your GitHub repository page, select the Settings tab, and then select Secrets and variables > Actions from the left menu. Define the following secrets that your workflow uses:
  • ACR_NAME: The ACR_Name value returned by the setup script
  • ACR_LOGIN: The ACR Login Username value returned by the setup script
  • ACR_PASSWORD: The ACR Login Password value returned by the setup script
  • RESOURCE_GROUP: The Resource Group Name value returned by the setup script
  • CLUSTER_NAME: contoso-video
To define each secret:
  1. Select New repository secret.
  2. For Name, enter the secret name from the preceding list.
  3. For Secret, enter the value you saved from the setup script, or run a Cloud Shell query to get the value.
  4. Select Add secret.
Run optional queries to get the secrets values
If you don't have the values the setup script returned, you can run the following commands in Azure Cloud Shell to get the information:
  • ACR_NAME:
Azure CLI 
• az acr list --query "[?contains(resourceGroup, 'mslearn-gh-pipelines')].loginServer" -o table
• ACR_LOGIN:
Azure CLI 
• az acr credential show --name <ACR_NAME> --query "username" -o table
• ACR_PASSWORD:
Azure CLI 
• az acr credential show --name <ACR_NAME> --query "passwords[0].value" -o table
• RESOURCE_GROUP:
Azure CLI 
  • az aks list -o tsv --query "[?name=='contoso-video'].resourceGroup"
Run the job
  1. Select the Actions tab.
  2. Select the only execution in the list, the failed build-staging.yml job.
  3. At upper right, select Re-run jobs > Re-run all jobs, and on the Re-run all jobs screen, select Re-run jobs.
  
  4. When the build completes, run az acr repository list --name <ACR_NAME> -o table in Cloud Shell to confirm that the contoso-website Container Registry repository appears in the results.
Continue to the next unit to build your production workflow.






Deploy with Helm
https://learn.microsoft.com/en-us/training/modules/aks-deployment-pipeline-github-actions/8-helm

Helm is an open-source packaging tool similar to Linux package managers like APT and Yum. Helm can help you install and manage the lifecycle of Kubernetes applications.

One of the advantages of using Helm is to not have to deploy files individually. You can issue a single command to deploy the chart. You can even deploy multiple dependent charts, with an automatic dependency resolution.


Look at the following example of the deployment.yaml file in the kubernetes directory of your website fork:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: contoso-website
spec:
  selector:
    matchLabels:
      app: contoso-website
  template:
    metadata:
      labels:
        app: contoso-website
    spec:
      containers:
        - image: !IMAGE!
          name: contoso-website
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 250m
              memory: 256Mi
          ports:
            - containerPort: 80
              name: http


To run the code manually, you can pipe the command to kubectl apply -f - to create the workloads:
$ sed 's+!IMAGE!+'"$ACR_NAME"'/contoso-website+g' kubernetes/deployment.yaml

run helm install to point to your image and pass your charts folder:
$ helm install contoso-website ./chart-location --set containerImage="$ACR_NAME/contoso-website"


1. What is Helm?
A: An open-source packaging tool that helps developers deploy and maintain Kubernetes applications.

2. What makes Helm especially useful in a CI/CD pipeline?
A: Helm provides several features that aren't included in Kubernetes charts, such as templating.
Besides templating, Helm also allows one-click deployment and provides a CLI to resolve chart dependencies, reducing the number of deployment steps needed.


Exercise - Create a Helm chart for deployment
complete the following tasks:
  • Check the Helm installation.
  • Create a chart.
  • Configure the chart.
  • Create a deployment.
  • Create an empty YAML file.
  • Add contents to the YAML file.
  • Create a service.
  • Create an ingress.
  • Create a DNS zone name.


Check the Helm installation.
1. Run helm version to make sure the displayed Helm version is greater than 3.
2. Switch to your forked repository for this module by running cd mslearn-aks-deployment-pipeline-github-actions.
3. Run git pull origin main to pull in your changes from previous units.


helm version
version.BuildInfo{Version:"v3.2.3", GitCommit:"8f832046e258e2cb800894579b1b3b50c2d83492", GitTreeState:"clean", GoVersion:"go1.13.12"}


Create a chart.
In Cloud Shell, switch to the kubernetes directory:
cd kubernetes

Use helm create to create a new directory called contoso-website in the kubernetes directory:
helm create contoso-website

Switch to the new directory with cd.
cd contoso-website

Delete the charts and templates folders in this directory.
rm -r charts templates


Create a new empty templates folder.
mkdir templates


To start building workloads in your empty chart, create a new set of YAML files by moving the existing files in kubernetes to the new templates folder:
mv ../*.yaml ./templates


Configure the chart
• Run cd ../.. to switch to the root of your repository.
• Run code . to open the code editor in the current directory.
• In the left menu, expand the kubernetes/contoso-website folder, and open the Chart.yaml file. Chart.yaml is the file that names the chart and is where Helm looks for information about the chart.
• Remove all the contents of the file except for the first three lines and the chart version, and edit the description so the file looks like this example:
YAML 
apiVersion: v2
name: contoso-website
description: Chart for the Contoso company website
version: 0.1.0
• Save the file by selecting the top right corner of the editor toolbar and then selecting Save, or by pressing Ctrl+S.


Create a deployment
• rom the left menu, open the deployment.yaml file in the kubernetes/templates folder.
• In the main metadata section, add a new key called namespace with the value {{ default "staging" .Release.Namespace }}. The metadata section should look like this example:
YAML 
• metadata:
  name: contoso-website
  namespace: {{ default "staging" .Release.Namespace }}

By default, the workflow deploys this resource to the staging namespace, but if the helm install command has a Namespace option, the workflow uses that namespace instead.
• Under the template/spec/containers section, replace !IMAGE! with the latest or tag versions from your AKS cluster.
It's good practice to split up the registry, image, and tag parts of the image name to work with them more easily. Add three new template variables that use the values of Values.image.registry, Values.image.name, and Values.image.tag.
- image: {{ .Values.image.registry }}.azurecr.io/{{ .Values.image.name }}:{{ default "latest" .Values.image.tag }}

Your deployment.yaml file should look like the following example:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: contoso-website
  namespace: {{ default "staging" .Release.Namespace }}
spec:
  selector:
    matchLabels:
      app: contoso-website
  template:
    metadata:
      labels:
        app: contoso-website
    spec:
      containers:
        - image: {{ .Values.image.registry }}.azurecr.io/{{ .Values.image.name }}:{{ default "latest" .Values.image.tag }}
          name: contoso-website
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 250m
              memory: 256Mi
          ports:
            - containerPort: 80
              name: http

Save the file.



• In the root of the contoso-website directory, open the values.yaml file.
• Delete all contents in the file, so you have an empty YAML file.
• Add the following content to the empty file, replacing the <ACR-NAME> placeholder with your Azure Container Registry name.
YAML 
image:
  registry: <ACR-NAME>
  name: contoso-website
  tag: latest


Create a service
  1. Open the service.yaml file in the templates folder.
  2. In the metadata section of the file, add a new key called namespace that uses the same value that you used in the deployment.yaml file.
YAML 
   namespace: {{ default "staging" .Release.Namespace }}
Your service.yaml file should look like the following example:
YAML 
• apiVersion: v1
kind: Service
metadata:
  name: contoso-website
  namespace: {{ default "staging" .Release.Namespace }}
spec:
  ports:
    - port: 80
      protocol: TCP
      targetPort: http
      name: http
  selector:
    app: contoso-website
  type: ClusterIP
• Save the file.



Create an ingress
  1. Open the ingress.yaml file.
  2. In the metadata section of the file, again add the namespace value that you used in the deployment.yaml file.
  3. Go to the host key. You create separate hosts for staging and production deployments, so users can't access the staging namespace by using production URLs. Concatenate the namespace in the host name. The HTTP application routing add-on in the AKS cluster handles name resolution.
YAML 
•        - host: contoso-{{ default "staging" .Release.Namespace }}.!DNS!
• Replace !DNS! with a new template variable for your DNS zone name.
YAML 
       - host: contoso-{{ default "staging" .Release.Namespace }}.{{ .Values.dns.name }}
Your final ingress.yaml file should look like the following example:
YAML 
• apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: contoso-website
  namespace: {{ default "staging" .Release.Namespace }}
  annotations:
    kubernetes.io/ingress.class: addon-http-application-routing
spec:
  rules:
    - host: contoso-{{ default "staging" .Release.Namespace }}.{{ .Values.dns.name }}
      http:
        paths:
          - backend:
              service:
                name: contoso-website
                port:
                  name: http
            path: /
            pathType: Prefix
• Save the file.



Add a DNS zone name value
  1. Open the values.yaml file and add a dns.name key after the image key. Replace <ACR-NAME> with your Container Registry name and <DNS-NAME> with your AKS DNS Zone Name from the setup script output. Your file should look like the following example:
YAML 
image:
  registry: <ACR-NAME>
  name: contoso-website
  tag: latest
dns:
  name: <DNS-NAME>
If you don't have your DNS zone name from the original setup script output, run the following Azure CLI query in a different Cloud Shell window to get it, replacing the <resource-group-name> placeholder with your resource group name.
Azure CLI 
  1. az aks show -g <resource-group-name> -n contoso-website -o tsv --query addonProfiles.httpApplicationRouting.config.HTTPApplicationRoutingZoneName
  2. Save the file.



Push your changes
Close the Cloud Shell Editor by selecting the upper right corner of the editor toolbar and then selecting Close Editor.
To push all the changes to your fork, run the following commands in Cloud Shell in order:
Bash 
git add .
Bash 
git commit -m "Add helm"
Bash 
git push -u origin main


















