Azure Kubernetes Service deployment pipeline with GitHub Actions
https://learn.microsoft.com/en-us/training/modules/aks-deployment-pipeline-github-actions/

• Describe a continuous integration and continuous deployment process that uses GitHub Actions.
• Create a deployment pipeline by using GitHub Actions and Azure.
• Deploy a cloud-native application to AKS by using GitHub Actions.


Suppose that you lead the IT development team for Contoso Video, a video production company whose technology stack has migrated to Azure Kubernetes Service (AKS). 


CI/CD pipelines

1. What's the difference between CI and CD?
A: CI is related to integration and testing, and CD is related to packaging and deployment.
CI is related to the integration of a team's code, and CD is related to the packaging and deployment of the code.

2. What's a CI pipeline?
A: A set of processes that runs automatically when a new code change is detected.
The set of processes that are automatically triggered by a code change is called a CI pipeline.


Design the pipeline
Triggers

Step 1: Clone the repo


Step 2: Build the image


Step 3: Push the image to a container registry


Step 4: Deploy the application




Exercise - Set up the project

1. Fork the sample repository to your GitHub account. You don't have write permissions to the original sample repository, so you need to fork the repository to create your own GitHub Actions pipeline.

https://github.com/MicrosoftDocs/mslearn-aks-deployment-pipeline-github-actions

https://github.com/jiasimon/mslearn-aks-deployment-pipeline-github-actions


2. Sign in to Azure Cloud Shell with the Azure subscription you want to deploy resources to. When Cloud Shell opens, make sure Bash is selected as the shell to run.
3. Clone 
git clone https://github.com/jiasimon/mslearn-aks-deployment-pipeline-github-actions

4. cd mslearn-aks-deployment-pipeline-github-actions

5. Run the following command to execute the init.sh file located in the root of the project:
bash init.sh

6. When the script finishes running, it outputs a list of variables. Copy and store the variable values to use in future exercises.
  • Resource Group Name
  • ACR Name
  • ACR Login Username
  • ACR Password
  • AKS DNS Zone Name


Run the following Azure CLI command to check whether the resource group shown in the script output is listed.
az group list -o table

Run the following command to check whether the Container Registry instance shown in the script output is listed.
az acr list -o table



Container images
Note: The docker build command doesn't work within the Cloud Shell environment, because the use of Docker inside a running container is disallowed. If you want to test the docker build . command, clone the repository locally and run the command by using your own Docker installation.

Image tags
Tags are an important aspect of working with container images. Tags can tell you the type of image and differentiate between multiple images that have the same name

1. What's a Docker image?
A: A standalone, lightweight package that includes all you need to build and run an application in a container.


2. How can a CI/CD pipeline use image tags?
A: To create and deploy different versions of application images that have the same name.




Exercise - Build the application image

Create the GitHub Actions workflow
• Select the Actions tab.
• Select the link to set up a workflow yourself.
At this point, the pipeline is just a blank file in the .github/workflows directory in your repository. GitHub provides the prebuilt components that you need to build most of the pipelines. To get started, copy and paste the following code into the Edit new file pane:
# This is a basic workflow to help you get started with Actions

name: CI

# Controls when the action will run. Triggers the workflow on push or pull request
# events but only for the main branch
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      # Runs a single command using the runners shell
      - name: Run a one-line script
        run: echo Hello, world!

      # Runs a set of commands using the runners shell
      - name: Run a multi-line script
        run: |
          echo Add other actions to build,
          echo test, and deploy your project.


Above the Edit new file pane, rename the file from main.yml to build-staging.yml.

Change the name key from CI to Build and push the latest build to staging.
# This is a basic workflow to help you get started with Actions

name: Build and push the latest build to staging


Modify the 'on' trigger
The basic workflow template comes with two triggers:
  • Any push to the main branch.
  • Any pull request on the main branch.


You don't need the pipeline to run on a pull request, so modify it to keep only the push trigger by changing the triggers in the on key. Remove the second trigger and leave only the push tags.

name: Build and push the latest build to staging

on:
  push:
    branches: [ main ]



Next, start working on the job steps. In this process, you implement both the build tasks and the deploy tasks in your pipeline design diagram.
  1. Under jobs, rename the build key to build_push_image.
  2. You want this workflow to run on Ubuntu 20.04, so change the runs-on key from ubuntu-latest to ubuntu-20.04.
  3. Delete the last two commands in the steps key, which are just examples for the template.
Your file, without the comments, should look like this example:
name: Build and push the latest build to staging

on:
  push:
    branches: [ main ]

jobs:
  build_push_image:
    runs-on: ubuntu-20.04

    steps:
      - uses: actions/checkout@v2



Add Docker steps
1. In the Marketplace tab in the right panel, search for docker login, and select the first result published by docker.
2. Under Installation, select the copy icon to copy the usage YAML.
3. Paste the copied YAML below the actions/checkout@v2 action.
Note: 
Be careful with indentation when you use YAML. The name key should be aligned with the preceding uses key.

4. Add the following values to the registry, username, and password keys:
• registry: ${{ secrets.ACR_NAME }}
• username: ${{ secrets.ACR_LOGIN }}
• password: ${{ secrets.ACR_PASSWORD }}

5. Delete the other keys, because they aren't used in this exercise.
6. In the right panel under Marketplace, search for build and push docker images, and select the first result published by docker.
7. Under Installation, select the copy icon to copy the usage YAML.
8. Paste the copied YAML below the last key from the previously copied docker-login action.
9. Rename the name key from Build and push Docker images to Build and push staging images.
10. Add the following values to the context, push, and tags keys:
  • context: .
  • push: true
  • tags: ${{secrets.ACR_NAME}}/contoso-website:latest
11. Delete the other keys, because they aren't used in this exercise.
12. Add another action called docker/setup-buildx-action between the checkout action and the login action, to set up the build engine for Docker to use. Copy the following snippet and paste it between the checkout and login actions.

- name: Set up Buildx
  uses: docker/setup-buildx-action@v3.0.0


To commit your changes, select the Commit changes button at upper right. On the Commit changes screen, enter a description for the commit, and then select Commit changes.

Set the secrets
To set the secrets, on your GitHub repository page, select the Settings tab, and then select Secrets and variables > Actions from the left menu. Define the following secrets that your workflow uses:
  • ACR_NAME: The ACR_Name value returned by the setup script
  • ACR_LOGIN: The ACR Login Username value returned by the setup script
  • ACR_PASSWORD: The ACR Login Password value returned by the setup script
  • RESOURCE_GROUP: The Resource Group Name value returned by the setup script
  • CLUSTER_NAME: contoso-video
To define each secret:
  1. Select New repository secret.
  2. For Name, enter the secret name from the preceding list.
  3. For Secret, enter the value you saved from the setup script, or run a Cloud Shell query to get the value.
  4. Select Add secret.
Run optional queries to get the secrets values
If you don't have the values the setup script returned, you can run the following commands in Azure Cloud Shell to get the information:
  • ACR_NAME:
Azure CLI 
• az acr list --query "[?contains(resourceGroup, 'mslearn-gh-pipelines')].loginServer" -o table
• ACR_LOGIN:
Azure CLI 
• az acr credential show --name <ACR_NAME> --query "username" -o table
• ACR_PASSWORD:
Azure CLI 
• az acr credential show --name <ACR_NAME> --query "passwords[0].value" -o table
• RESOURCE_GROUP:
Azure CLI 
  • az aks list -o tsv --query "[?name=='contoso-video'].resourceGroup"
Run the job
  1. Select the Actions tab.
  2. Select the only execution in the list, the failed build-staging.yml job.
  3. At upper right, select Re-run jobs > Re-run all jobs, and on the Re-run all jobs screen, select Re-run jobs.
  
  4. When the build completes, run az acr repository list --name <ACR_NAME> -o table in Cloud Shell to confirm that the contoso-website Container Registry repository appears in the results.
Continue to the next unit to build your production workflow.






Deploy with Helm
https://learn.microsoft.com/en-us/training/modules/aks-deployment-pipeline-github-actions/8-helm

Helm is an open-source packaging tool similar to Linux package managers like APT and Yum. Helm can help you install and manage the lifecycle of Kubernetes applications.

One of the advantages of using Helm is to not have to deploy files individually. You can issue a single command to deploy the chart. You can even deploy multiple dependent charts, with an automatic dependency resolution.


Look at the following example of the deployment.yaml file in the kubernetes directory of your website fork:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: contoso-website
spec:
  selector:
    matchLabels:
      app: contoso-website
  template:
    metadata:
      labels:
        app: contoso-website
    spec:
      containers:
        - image: !IMAGE!
          name: contoso-website
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 250m
              memory: 256Mi
          ports:
            - containerPort: 80
              name: http


To run the code manually, you can pipe the command to kubectl apply -f - to create the workloads:
$ sed 's+!IMAGE!+'"$ACR_NAME"'/contoso-website+g' kubernetes/deployment.yaml

run helm install to point to your image and pass your charts folder:
$ helm install contoso-website ./chart-location --set containerImage="$ACR_NAME/contoso-website"


1. What is Helm?
A: An open-source packaging tool that helps developers deploy and maintain Kubernetes applications.

2. What makes Helm especially useful in a CI/CD pipeline?
A: Helm provides several features that aren't included in Kubernetes charts, such as templating.
Besides templating, Helm also allows one-click deployment and provides a CLI to resolve chart dependencies, reducing the number of deployment steps needed.


Exercise - Create a Helm chart for deployment
complete the following tasks:
  • Check the Helm installation.
  • Create a chart.
  • Configure the chart.
  • Create a deployment.
  • Create an empty YAML file.
  • Add contents to the YAML file.
  • Create a service.
  • Create an ingress.
  • Create a DNS zone name.


Check the Helm installation.
1. Run helm version to make sure the displayed Helm version is greater than 3.
2. Switch to your forked repository for this module by running cd mslearn-aks-deployment-pipeline-github-actions.
3. Run git pull origin main to pull in your changes from previous units.


helm version
version.BuildInfo{Version:"v3.2.3", GitCommit:"8f832046e258e2cb800894579b1b3b50c2d83492", GitTreeState:"clean", GoVersion:"go1.13.12"}


Create a chart.
In Cloud Shell, switch to the kubernetes directory:
cd kubernetes

Use helm create to create a new directory called contoso-website in the kubernetes directory:
helm create contoso-website

Switch to the new directory with cd.
cd contoso-website

Delete the charts and templates folders in this directory.
rm -r charts templates


Create a new empty templates folder.
mkdir templates


To start building workloads in your empty chart, create a new set of YAML files by moving the existing files in kubernetes to the new templates folder:
mv ../*.yaml ./templates


Configure the chart
• Run cd ../.. to switch to the root of your repository.
• Run code . to open the code editor in the current directory.
• In the left menu, expand the kubernetes/contoso-website folder, and open the Chart.yaml file. Chart.yaml is the file that names the chart and is where Helm looks for information about the chart.
• Remove all the contents of the file except for the first three lines and the chart version, and edit the description so the file looks like this example:
YAML 
apiVersion: v2
name: contoso-website
description: Chart for the Contoso company website
version: 0.1.0
• Save the file by selecting the top right corner of the editor toolbar and then selecting Save, or by pressing Ctrl+S.


Create a deployment
• rom the left menu, open the deployment.yaml file in the kubernetes/templates folder.
• In the main metadata section, add a new key called namespace with the value {{ default "staging" .Release.Namespace }}. The metadata section should look like this example:
YAML 
• metadata:
  name: contoso-website
  namespace: {{ default "staging" .Release.Namespace }}

By default, the workflow deploys this resource to the staging namespace, but if the helm install command has a Namespace option, the workflow uses that namespace instead.
• Under the template/spec/containers section, replace !IMAGE! with the latest or tag versions from your AKS cluster.
It's good practice to split up the registry, image, and tag parts of the image name to work with them more easily. Add three new template variables that use the values of Values.image.registry, Values.image.name, and Values.image.tag.
- image: {{ .Values.image.registry }}.azurecr.io/{{ .Values.image.name }}:{{ default "latest" .Values.image.tag }}

Your deployment.yaml file should look like the following example:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: contoso-website
  namespace: {{ default "staging" .Release.Namespace }}
spec:
  selector:
    matchLabels:
      app: contoso-website
  template:
    metadata:
      labels:
        app: contoso-website
    spec:
      containers:
        - image: {{ .Values.image.registry }}.azurecr.io/{{ .Values.image.name }}:{{ default "latest" .Values.image.tag }}
          name: contoso-website
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 250m
              memory: 256Mi
          ports:
            - containerPort: 80
              name: http

Save the file.



• In the root of the contoso-website directory, open the values.yaml file.
• Delete all contents in the file, so you have an empty YAML file.
• Add the following content to the empty file, replacing the <ACR-NAME> placeholder with your Azure Container Registry name.
YAML 
image:
  registry: <ACR-NAME>
  name: contoso-website
  tag: latest


Create a service
  1. Open the service.yaml file in the templates folder.
  2. In the metadata section of the file, add a new key called namespace that uses the same value that you used in the deployment.yaml file.
YAML 
   namespace: {{ default "staging" .Release.Namespace }}
Your service.yaml file should look like the following example:
YAML 
• apiVersion: v1
kind: Service
metadata:
  name: contoso-website
  namespace: {{ default "staging" .Release.Namespace }}
spec:
  ports:
    - port: 80
      protocol: TCP
      targetPort: http
      name: http
  selector:
    app: contoso-website
  type: ClusterIP
• Save the file.



Create an ingress
  1. Open the ingress.yaml file.
  2. In the metadata section of the file, again add the namespace value that you used in the deployment.yaml file.
  3. Go to the host key. You create separate hosts for staging and production deployments, so users can't access the staging namespace by using production URLs. Concatenate the namespace in the host name. The HTTP application routing add-on in the AKS cluster handles name resolution.
YAML 
•        - host: contoso-{{ default "staging" .Release.Namespace }}.!DNS!
• Replace !DNS! with a new template variable for your DNS zone name.
YAML 
       - host: contoso-{{ default "staging" .Release.Namespace }}.{{ .Values.dns.name }}
Your final ingress.yaml file should look like the following example:
YAML 
• apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: contoso-website
  namespace: {{ default "staging" .Release.Namespace }}
  annotations:
    kubernetes.io/ingress.class: addon-http-application-routing
spec:
  rules:
    - host: contoso-{{ default "staging" .Release.Namespace }}.{{ .Values.dns.name }}
      http:
        paths:
          - backend:
              service:
                name: contoso-website
                port:
                  name: http
            path: /
            pathType: Prefix
• Save the file.



Add a DNS zone name value
  1. Open the values.yaml file and add a dns.name key after the image key. Replace <ACR-NAME> with your Container Registry name and <DNS-NAME> with your AKS DNS Zone Name from the setup script output. Your file should look like the following example:
YAML 
image:
  registry: <ACR-NAME>
  name: contoso-website
  tag: latest
dns:
  name: <DNS-NAME>
If you don't have your DNS zone name from the original setup script output, run the following Azure CLI query in a different Cloud Shell window to get it, replacing the <resource-group-name> placeholder with your resource group name.
Azure CLI 
  1. az aks show -g <resource-group-name> -n contoso-website -o tsv --query addonProfiles.httpApplicationRouting.config.HTTPApplicationRoutingZoneName
  2. Save the file.



Push your changes
Close the Cloud Shell Editor by selecting the upper right corner of the editor toolbar and then selecting Close Editor.
To push all the changes to your fork, run the following commands in Cloud Shell in order:
Bash 
git add .
Bash 
git commit -m "Add helm"
Bash 
git push -u origin main







Exercise - Create the deployment pipeline
https://learn.microsoft.com/en-us/training/modules/aks-deployment-pipeline-github-actions/10-exercise-deploy-workflow

The deployment steps include:
  • Create the deploy job.
  • Set up Open ID Connect (OIDC).
  • Deploy the application with Helm.
  • Run the deployment on production.

Add the deploy job
  1. In GitHub, go to your fork of the repository.
  2. Expand the .github/workflows directory and open the build-staging.yml file for editing.
  3. Add a new deploy job at the end of the file, after the build_push_image job, as follows. Be sure to match the indentation.
The job has three keys: runs-on, needs, and permissions.
    ○ For runs-on, use ubuntu-20.04 to be consistent with the other job.
    ○ For needs, use the name of the first job, build_push_image, so the application only deploys after the image is built
    ○ For permissions, add two arguments called id-token and contents. Set id-token to write and contents to read, to grant GitHub Actions access to send requests and read the repo contents.
  4. Add - uses: actions/checkout@v2 as the first step of the job.
The added deploy job should look like the following code:
deploy:
        runs-on: ubuntu-20.04
        needs: build_push_image
        permissions:
          id-token: write
          contents: read

        steps:
          - uses: actions/checkout@v2



Add the Install Helm step
Use a GitHub action to download and install Helm version v3.3.1.
  1. In the right panel of the editing page, search for helm tool installer. Select the first result published by Azure.
  
  2. Select the copy icon to copy the usage YAML.
  
  3. Copy and paste the YAML below the uses key in build-staging.yml.
  4. Rename the step from Helm tool installer to Install Helm, and pin the version key to v3.3.1.
YAML 
    steps:
      - uses: actions/checkout@v2
- name: Install Helm
        uses: Azure/setup-helm@v1
        with:
          version: v3.3.1



Add the Azure Login authentication step
Use OIDC to authenticate GitHub Actions to access AKS.
  1. In the right panel, search for Azure login, and select Azure Login published by Azure.
  
  2. Select the copy icon to copy the usage YAML, and paste it below the Install Helm step in build-staging.yml.
  3. Change the step name from Azure Login to Sign in to Azure with OIDC.
  4. Azure Login requires three parameters to authenticate: client-id, tenant-id, and subscription-id. Fill these parameters with placeholders for secrets you set later.
YAML 
•       - name: Sign in to Azure with OIDC
        uses: Azure/login@v1.5.1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
• In the right panel, search for set context, and select Azure Kubernetes set context published by Azure.

• Select the copy icon to copy the usage YAML, and paste it below the Sign in to Azure with OIDC step in build-staging.yml. Fill the resource-group and cluster-name parameters with placeholders for the secrets you set in an earlier unit.
YAML 
      - name: Azure Kubernetes set context
        uses: Azure/aks-set-context@v3
        with:
          resource-group: ${{ secrets.RESOURCE_GROUP }}
          cluster-name: ${{ secrets.CLUSTER_NAME }}
Your build-staging.yml file should look like the following example:
YAML 
name: Build and push the latest build to staging
on:
      push:
        branches: [ main ]
jobs:
      build_push_image:
        runs-on: ubuntu-20.04
steps:
          - uses: actions/checkout@v2
- name: Set up Buildx
            uses: docker/setup-buildx-action@v3.0.0
- name: Docker Login
            uses: docker/login-action@v3.0.0
            with:
              registry: ${{ secrets.ACR_NAME }}
              username: ${{ secrets.ACR_LOGIN }}
              password: ${{ secrets.ACR_PASSWORD }}
- name: Build and push staging images
            uses: docker/build-push-action@v5.0.0
            with:
              context: .
              push: true
              tags: ${{secrets.ACR_NAME}}/contoso-website:latest
deploy:
        runs-on: ubuntu-20.04
        needs: build_push_image # Will wait for the execution of the previous job
        permissions:
          id-token: write # This is required for requesting the JWT
          contents: read  # This is required for actions/checkout
steps:
          - uses: actions/checkout@v2
- name: Install Helm
            uses: Azure/setup-helm@v1
            with:
              version: v3.3.1
- name: Sign in to Azure with OIDC
            uses: Azure/login@v1.5.1
            with:
              client-id: ${{ secrets.AZURE_CLIENT_ID }}
              tenant-id: ${{ secrets.AZURE_TENANT_ID }}
              subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
- name: Azure Kubernetes set context
            uses: Azure/aks-set-context@v3
            with:
              resource-group: ${{ secrets.RESOURCE_GROUP }}
              cluster-name: ${{ secrets.CLUSTER_NAME }}



Set up Open ID Connect (OIDC)
Create the service principal
  1. In Azure Cloud Shell, run az account show, and save the id value from the output.
  2. Create a service principal by running the following command, substituting the id value from the previous command for $SUBSCRIPTION_ID:
Azure CLI 
az ad sp create-for-rbac --scopes /subscriptions/$SUBSCRIPTION_ID --role Contributor 


Set the secrets
On your GitHub repository page, select the Settings tab, and then select Secrets and variables > Actions from the left menu. Define the following three new secrets that use the output from the preceding steps.
  • AZURE_CLIENT_ID: The "appId" value from az ad sp create-for-rbac output
  • AZURE_TENANT_ID: The "tenant" value from az ad sp create-for-rbac output
  • AZURE_SUBSCRIPTION_ID: The id value from az account show output
For each secret:
  1. Select New repository secret.
  2. For Name, enter the secret name.
  3. For Secret, enter the value.
  4. Select Add secret.


Add federated credentials
Create federated certificates to authorize GitHub Actions to access the application.
  1. In the Azure portal, go to App registrations.
  2. Search for and select the application that matches the displayName value returned in the previous az ad sp create-for-rbac step. By default, the application name uses the timestamp of the service principal creation.
  3. Verify that the values of the appID (Client ID), Object ID (Application Object ID), and Directory ID (Tenant ID) match the previous JSON output.
  4. In the left navigation, select Certificates & secrets.
  5. On the Certificates & secrets screen, select the Federated credentials tab.
  6. Select Add credential.
  7. To add the staging credential, on the Add a credential screen, select or enter the following information:
    ○ Federated credential scenario: Select GitHub Actions deploying Azure resources.
    ○ Organization: Enter your GitHub user name.
    ○ Repository: Enter mslearn-aks-deployment-pipeline-github-actions.
    ○ Entity type: Select Branch.
    ○ GitHub branch name: Enter main.
    ○ Name: Enter staging-cred.
    ○ Description Enter Testing.
  8. Select Add.
  9. To add the production credential, select Add credential again, and on the Add a credential screen, enter all the same values as for the previous credential except:
  • Entity type: Select Tag.
  • GitHub tag name: Enter v2.0.0, because in the next step you deploy version 2.
  • Name: Enter prod-cred.
  
  10. Select Add.
  


Deploy the application with Helm
Now that you configured Helm and granted access to your cluster, you're ready to deploy the application.
Add the Run Helm Deploy step
  1. Back in the build-staging.yml file in GitHub, after the latest step in the deploy job, create a new step named Run Helm Deploy. Below it, add another key called run.
YAML 
•           - name: Run Helm Deploy
            run:
• You can use the run key to execute any shell command inside the container. This pipeline uses the run key to execute the following Helm command:
Bash 
helm upgrade --install --create-namespace --atomic --wait 
    --namespace staging contoso-website \
    ./kubernetes/contoso-website \
    --set image.repository=${{ secrets.ACR_NAME }} \
    --set dns.name=${{ secrets.DNS_NAME }}


Add the command to the file and set it to run, starting with the | character. The Run Helm deploy step should match this example:
YAML 
  ...
      - name: Run Helm Deploy
        run: |
          helm upgrade \
            --install \
            --create-namespace \
            --atomic \
            --wait \
            --namespace staging \
            contoso-website \
            ./kubernetes/contoso-website \
            --set image.repository=${{ secrets.ACR_NAME }} \
            --set dns.name=${{ secrets.DNS_NAME }}
Your completed build-staging.yml file should look like the following example:
YAML 
name: Build and push the latest build to staging
on:
      push:
        branches: [ main ]
jobs:
      build_push_image:
        runs-on: ubuntu-20.04
steps:
          - uses: actions/checkout@v2
- name: Set up Buildx
            uses: docker/setup-buildx-action@v3.0.0
- name: Docker Login
            uses: docker/login-action@v3.0.0
            with:
              registry: ${{ secrets.ACR_NAME }}
              username: ${{ secrets.ACR_LOGIN }}
              password: ${{ secrets.ACR_PASSWORD }}
- name: Build and push staging images
            uses: docker/build-push-action@v5.0.0
            with:
              context: .
              push: true
              tags: ${{secrets.ACR_NAME}}/contoso-website:latest
deploy:
        runs-on: ubuntu-20.04
        needs: build_push_image # Waits for the execution of the previous job
        permissions:
          id-token: write # Required for requesting the JWT
          contents: read  # Required for actions/checkout
steps:
          - uses: actions/checkout@v2
- name: Install Helm
            uses: Azure/setup-helm@v1
            with:
              version: v3.3.1
- name: Sign in to Azure with OIDC
            uses: Azure/login@v1.5.1
            with:
              client-id: ${{ secrets.AZURE_CLIENT_ID }}
              tenant-id: ${{ secrets.AZURE_TENANT_ID }}
              subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
- name: Azure Kubernetes set context
            uses: Azure/aks-set-context@v3
            with:
              resource-group: ${{ secrets.RESOURCE_GROUP }}
              cluster-name: ${{ secrets.CLUSTER_NAME }}
- name: Run Helm Deploy
            run: |
              helm upgrade \
                --install \
                --create-namespace \
                --atomic \
                --wait \
                --namespace staging \
                contoso-website \
                ./kubernetes/contoso-website \
                --set image.repository=${{ secrets.ACR_NAME }} \
                --set dns.name=${{ secrets.DNS_NAME }}


Set the DNS_NAME secret
  1. In a new browser tab, go to your fork of the repository, select the Settings tab, and then select Secrets and variables > Actions from the left menu.
  2. Select New repository secret.
  3. For Name, enter DNS_NAME.
  4. For Secret, enter the AKS DNS Zone Name value from your original setup script output.
If you don't have this value, run the following command in Cloud Shell, substituting your values for <resource-group-name> and <aks-cluster-name>:
Azure CLI 
• az aks show -g <resource-group-name> -n <aks-cluster-name> -o tsv --query addonProfiles.httpApplicationRouting.config.HTTPApplicationRoutingZoneName
• Select Add secret.



Commit the changes and test the staging deployment
  1. To commit your changes, select Commit changes. Enter a description for the commit, and then select Commit changes.
  2. Select the Actions tab to see the build running.
  3. After the build succeeds, in your browser go to contoso-staging.<aks-dns-zone-name> to confirm that the website appears.


Run the deployment on production
The next step is to create the production workflow.
  1. In the .github/workflows directory in your repository, open the build-production.yml file for editing.
  2. Copy the deploy job from the staging pipeline and paste it below the last line in the build-production.yml file.
  3. Change the Run Helm Deploy step to deploy to the production namespace by changing the --namespace flag from staging to production.
  4. At the end of the Helm command, add a new parameter, --set image.tag=${GITHUB_REF##*/}.
Here, you use a Bash feature called parameter expansion. The expansion ${ENV##<wildcard><character>} returns the last occurrence of the string after character.
In this case, you want to get only the tag name, which is represented as the GitHub Actions runtime, GITHUB_REF. Branches are refs/heads/<branch>, while tags are refs/tags/<tag>.
You want to remove refs/tags/ to get only the tag name, so you pass ${GITHUB_REF##*/} to return everything after the last / in the GITHUB_REF environment variable.
Your final build-production.yml file should look like the following example:


Production changes
Every time you run the production workflow, you need to update the federated certificate with the corresponding tag version, as follows:
  1. In the Azure portal, go to your application page and select Certificates & secrets in the left navigation.
  2. Select the Federated credentials tab.
  3. Select the prod-cred credential.
  4. On the Edit a credential screen, next to Based on selection, increment the tag number to a new v.x.x.x such as v.2.0.1.
  5. Select Update.
  6. In Cloud Shell, run git pull to fetch the latest changes. Then, run the following command to tag and push the changes, substituting your new version tag for the placeholder:
Bash 
• git tag -a v<new version tag> -m 'Create new production deployment' && git push --tags
• When prompted, provide the PAT from previous exercises as the password.
• In GitHub, open the Actions tab and see the running process.
• After the workflow succeeds, to test the production deployment, go to contoso-production.<aks-dns-zone-name> in your browser and confirm that the website appears.


Clean up resources
Azure resources
  1. In the Azure portal, search for and select resource groups, and then select the resource group you used for this module from the list.
  2. On the Overview page, select Delete resource group.
  3. To confirm the deletion and delete all the resources you created in this module, enter the name of the resource group, and select Delete.
  4. Repeat the preceding steps for the corresponding resource group that starts with MC_mslearn-gh-pipelines.
  5. Go to App registrations and select your app from the list.
  6. On the Overview page, select Delete.
  7. On the Delete app registration page, select Delete, which also deletes the federated credentials.
GitHub
  1. Go to your fork of the Contoso Video repository.
  2. Select the Settings tab.
  3. Scroll down and select the red Delete this repository button. To confirm the deletion, enter the full name of the repository.
  4. To delete the PAT you created, select your profile photo, then select Settings.
  5. Select Developer settings.
  6. Select Personal access tokens.
  7. Select Delete. When the pop-up window appears, select I understand, delete this token.

















